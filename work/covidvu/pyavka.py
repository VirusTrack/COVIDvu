#!/usr/bin/env python3
# See: https://github.com/pr3d4t0r/COVIDvu/blob/master/LICENSE 
# vim: set fileencoding=utf-8:


from bs4 import BeautifulSoup

from covidvu.vujson import SITE_DATA
from covidvu.vudpatch import SCRAPED_US_DATA
from covidvu.vudpatch import SCRAPED_WORLD_DATA

import csv
import os


# --- constants ---

NIXED_ROWS_INDEX = (
    'American Samoa',
    'Diamond Princess',
    'Grand Princess',
    'Guam',
    'Northern Mariana Islands',
    'Puerto Rico',
    'Queue',
    'TBD',
    'TOTAL',
    'U.S. TOTAL',
    'U.S. Virgin Islands',
    'Wuhan',
)
US_TABLE_HTML    = os.path.join(SITE_DATA, 'table-01.html')
WORLD_TABLE_HTML = os.path.join(SITE_DATA, 'table-00.html')


# +++ functions +++

def _generateCSVTo(targetFileName, dataSource = WORLD_TABLE_HTML, ignoreRows = 6):
    """
    targetFileName ::= where to write the CSV
    dataSource     ::= the scraped HTML file generated by pyavka.sh
    ignoreRows     ::= heuristic for skipping the first row headers in the
                       dataSource.
    """
    rowCount = 0
    rows     = list()
    soup     = BeautifulSoup(open(dataSource).read(), 'html.parser')
    table    = soup.find('table')

    print('processing %s...' % dataSource)

    for tableRow in table.find_all('tr'):
        rowCount += 1

        if rowCount > ignoreRows:
            row     = list()
            for column in tableRow.find_all('td'):
                row.append(column.text.replace(',', '')) # number format comma
            
            if any(location in row for location in NIXED_ROWS_INDEX) or not len(row[0]):
                continue    # skip to next record
            
            rows.append(row)

    with open(targetFileName, 'w') as outputFile:
        csv.writer(outputFile, delimiter = '\t').writerows(rows)

    print('generated %s' % targetFileName)


def _main():
    _generateCSVTo(SCRAPED_WORLD_DATA, WORLD_TABLE_HTML, 6)
    _generateCSVTo(SCRAPED_US_DATA, US_TABLE_HTML, 5)


# --- main ---

if '__main__' == __name__:
    _main()

